{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Battle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note.** The `@threads` cell in this notebook takes about 4 hours to complete on a\n",
    "2018 MacBook Pro:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the predictive performance of several tumor growth models on data collected\n",
    "in Laleh et al. [(2022)](https://doi.org/10.1371/journal.pcbi.1009822) \"Classical\n",
    "mathematical models for prediction of response to chemotherapy and immunotherapy\", *PLOS\n",
    "Computational Biology*\". In particular, we determine whether differences observed are\n",
    "statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to classical models, we include a 2D generalization of the generalized\n",
    "Bertalanffy model, `bertalanffy2`, and some 1D and 2D neural ODE's. The 2D models still\n",
    "model a single lesion feature, namely it's volume, but add a second latent variable\n",
    "coupled to the volume, effectively making the model second order. For further details,\n",
    "refer to the TumorGrowth.jl [package\n",
    "documentation](https://ablaom.github.io/TumorGrowth.jl/dev/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We needed to eliminate about 10% of patient records because of failure of the neural\n",
    "network models to converge before parameters went out of bounds. A bootstrap comparison\n",
    "of the differences in mean absolute errors suggest that the generalized Bertalanffy model\n",
    "performs significantly better than all other models, with the exception of the 1D neural\n",
    "ODE. However, in pair-wise comparisons the neural ODE model was *not* significantly\n",
    "better than any model. Results are summarised in the table below. Arrows point to\n",
    "bootstrap winners in the top row or first column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                            | **logistic** | **classical\\_bertalanffy** | **bertalanffy** | **bertalanffy2** | **1D neural** | **2D neural** |\n",
    "|---------------------------:|-------------:|---------------------------:|----------------:|-----------------:|--------------:|--------------:|\n",
    "|               **gompertz** |         draw |                       draw |               ↑ |             draw |          draw |             ← |\n",
    "|               **logistic** |          n/a |                       draw |               ↑ |             draw |          draw |             ← |\n",
    "| **classical\\_bertalanffy** |         draw |                        n/a |               ↑ |             draw |          draw |             ← |\n",
    "|            **bertalanffy** |            ← |                          ← |             n/a |                ← |          draw |             ← |\n",
    "|           **bertalanffy2** |         draw |                       draw |               ↑ |              n/a |          draw |             ← |\n",
    "|              **1D neural** |         draw |                       draw |            draw |             draw |           n/a |             ← |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "dir = @__DIR__\n",
    "Pkg.activate(dir)\n",
    "Pkg.instantiate()\n",
    "\n",
    "using Random\n",
    "using Statistics\n",
    "using TumorGrowth\n",
    "using Lux\n",
    "using Plots\n",
    "import PrettyPrint.pprint\n",
    "using PrettyTables\n",
    "using Bootstrap\n",
    "using Serialization\n",
    "using ProgressMeter\n",
    "using .Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect together all records with at least 6 measurements, from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = filter(patient_data()) do record\n",
    "    record.readings >= 6\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what a single record looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(records[13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural ODEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some one and two-dimensional neural ODE models we want to include in our\n",
    "comparison. The choice of architecture here is somewhat ad hoc and further\n",
    "experimentation might give better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Chain(\n",
    "    Dense(1, 3, Lux.tanh, init_weight=Lux.zeros64),\n",
    "    Dense(3, 1),\n",
    ")\n",
    "\n",
    "network2 = Chain(\n",
    "    Dense(2, 2, Lux.tanh, init_weight=Lux.zeros64),\n",
    "    Dense(2, 2),\n",
    ")\n",
    "\n",
    "n1 = neural(Xoshiro(123), network) # `Xoshiro` is a random number generator\n",
    "n2 = neural2(Xoshiro(123), network2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models to be compared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_exs =\n",
    "    [:gompertz, :logistic, :classical_bertalanffy, :bertalanffy, :bertalanffy2, :n1, :n2]\n",
    "models = eval.(model_exs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing prediction errors on a holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdouts = 2\n",
    "recs = records;\n",
    "errors = fill(Inf, length(recs), length(models))\n",
    "\n",
    "p = Progress(length(recs))\n",
    "\n",
    "@threads for i in eachindex(recs)\n",
    "    record = records[i]\n",
    "    times, volumes = record.T_weeks, record.Lesion_normvol\n",
    "    comparison = compare(times, volumes, models; holdouts, flag_out_of_bounds=true)\n",
    "    errors[i,:] = TumorGrowth.errors(comparison)\n",
    "    next!(p)\n",
    "end\n",
    "finish!(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialize(joinpath(dir, \"errors.jls\"), errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap comparisons (neural ODE's excluded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the neural ODE errors contain more `NaN` values (because of parameters going out\n",
    "of bounds), we start with a comparison that excludes them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_error_rows = filter(axes(errors, 1)) do i\n",
    "    es = errors[i,1:5]\n",
    "    any(isnan, es) || any(isinf, es) || max(es...) > 0.1\n",
    "end\n",
    "proportion_bad = length(bad_error_rows)/size(errors, 1)\n",
    "@show proportion_bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's less than 2%. Let's remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_error_rows = setdiff(axes(errors, 1), bad_error_rows);\n",
    "errors = errors[good_error_rows,:];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Errors are evidently not normally distributed (and we were not able to transform them\n",
    "to approximately normal):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = histogram(errors[:, 1], normalize=:pdf, alpha=0.4)\n",
    "histogram!(errors[:, 5], normalize=:pdf, alpha=0.4)\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We deem a student t-test inappopriate and instead compute bootstrap confidence intervals for pairwise differences in model errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_intervals = Array{Any}(undef, length(models) - 2, length(models) - 2)\n",
    "for i in 1:(length(models) - 2)\n",
    "    for j in 1:(length(models) - 2)\n",
    "        b = bootstrap(\n",
    "            mean,\n",
    "            errors[:,i] - errors[:,j],\n",
    "            BasicSampling(10000),\n",
    "        )\n",
    "        confidence_intervals[i,j] = only(confint(b, BasicConfInt(0.95)))[2:3]\n",
    "    end\n",
    "end\n",
    "confidence_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret the confidence intervals as  follows:\n",
    "\n",
    "- if both endpoints -ve, row index wins\n",
    "\n",
    "- if both endpoints +ve, column index wins\n",
    "\n",
    "- otherwise a draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winner_pointer(ci) = ci == (0, 0) ? \"n/a\" :\n",
    "    isnan(first(ci)) && isnan(last(ci)) ? \"inconclusive\" :\n",
    "    first(ci) < 0 && last(ci) < 0 ? \"←\" :\n",
    "    first(ci) > 0 && last(ci) > 0 ? \"↑\" :\n",
    "    \"draw\"\n",
    "\n",
    "tabular(A, model_exs) = NamedTuple{(:model, model_exs[2:end]...)}((\n",
    "    model_exs[1:end-1],\n",
    "    (A[1:end-1, j] for j in 2:length(model_exs))...,\n",
    "))\n",
    "\n",
    "pretty_table(\n",
    "    tabular(winner_pointer.(confidence_intervals), model_exs[1:5]),\n",
    "    show_subheader=false,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap comparison of errors (neural ODE's included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_error_rows = filter(axes(errors, 1)) do i\n",
    "    es = errors[i,:]\n",
    "    any(isnan, es) || any(isinf, es) || max(es...) > 0.1\n",
    "end\n",
    "proportion_bad = length(bad_error_rows)/size(errors, 1)\n",
    "@show proportion_bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the additional 10%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_error_rows = setdiff(axes(errors, 1), bad_error_rows);\n",
    "errors = errors[good_error_rows,:];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And proceed as before, but with all columns of `errors` (all models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_intervals = Array{Any}(undef, length(models), length(models))\n",
    "for i in 1:length(models)\n",
    "    for j in 1:length(models)\n",
    "        b = bootstrap(\n",
    "            mean,\n",
    "            errors[:,i] - errors[:,j],\n",
    "            BasicSampling(10000),\n",
    "        )\n",
    "        confidence_intervals[i, j] = only(confint(b, BasicConfInt(0.95)))[2:3]\n",
    "    end\n",
    "end\n",
    "pretty_table(\n",
    "    tabular(winner_pointer.(confidence_intervals), model_exs),\n",
    "    show_subheader=false,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
