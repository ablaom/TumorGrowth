var documenterSearchIndex = {"docs":
[{"location":"comparison/#Model-comparison","page":"Model comparison","title":"Model comparison","text":"","category":"section"},{"location":"comparison/","page":"Model comparison","title":"Model comparison","text":"A simple model comparison example appears below. For an extended application, see Model Battle.","category":"page"},{"location":"comparison/","page":"Model comparison","title":"Model comparison","text":"compare","category":"page"},{"location":"comparison/#TumorGrowth.compare","page":"Model comparison","title":"TumorGrowth.compare","text":"compare(times, volumes, models; holdouts=3, metric=mae, advanced_options...)\n\nBy calibrating models using the specified patient times and lesion volumes, compare those models using a hold-out set consisting of the last holdouts data points.\n\ntimes = [0.1, 6.0, 16.0, 24.0, 32.0, 39.0]\nvolumes = [0.00023, 8.4e-5, 6.1e-5, 4.3e-5, 4.3e-5, 4.3e-5]\n\njulia> comparison = compare(times, volumes, [gompertz, logistic])\nModelComparison with 3 holdouts:\n  metric: mae\n  gompertz:     2.198e-6\n  logistic:     6.55e-6\n\njulia> errors(comparison)\n2-element Vector{Float64}:\n 2.197843662660861e-6\n 6.549858321487298e-6\n\njulia> p = parameters(comparison)[1]  # calibrated parameter for `gompertz`\n(v0 = 0.00022643603114569068, v∞ = 3.8453274218216947e-5, ω = 0.11537512108224635)\n\njulia> gompertz(times, p)\n6-element Vector{Float64}:\n 0.00022643603114569068\n 9.435316392754094e-5\n 5.1039159299783234e-5\n 4.303209015899451e-5\n 4.021112910411027e-5\n 3.922743006690166e-5\n\nVisualising comparisons\n\nusing Plots\nplot(comparison, title=\"A comparison of two models\")\n\nKeyword options\n\nholdouts=3: number of time-volume pairs excluded from the end of the calibration data\nmetric=mae: metric applied to holdout set; the reported error on a model predicting volumes v̂ is metric(v̂, v) where v is the last holdouts values of volumes. For example, any regression measure from StatisticalMeasures.jl can be used here. The built-in fallback is mean absolute error.\niterations=TumorGrowth.iterations.(models): a vector of iteration counts for the calibration of models\ncalibration_options: a vector of named tuples providing keyword arguments for the CalibrationProblem for each model. Possible keys are: p0, lower, upper, frozen, learning_rate, optimiser, radius, scale, half_life, penalty, and keys corresponding to any ODE solver options. Keys left unspecified fall back to defaults, as these are described in the CalibrationProblem document string.\n\nSee also errors, parameters.\n\n\n\n\n\n","category":"function"},{"location":"installation/#Installation","page":"Installation","title":"Installation","text":"","category":"section"},{"location":"installation/","page":"Installation","title":"Installation","text":"TumorGrowth.jl can be installed in any Julia package environment. When trying TumorGrowth.jl out for the first time, we recommend installing it in a fresh environment, as shown below.","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"using Pkg\nPkg.activate(\"my_oncology_project\", shared=true)\nPkg.add(\"TumorGrowth\")\nPkg.add(\"Plots\")","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"You should now be able to run the code in the Quick start or Extended examples sections.","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"In a new julia session you can re-activate the environment created above with:","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"using Pkg\nPkg.activate(\"my_oncology_project\", shared=true)","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"EditURL = \"notebook.jl\"","category":"page"},{"location":"examples/04_model_battle/notebook/#Model-Battle","page":"Model battle","title":"Model Battle","text":"","category":"section"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"The analysis below is also available in notebook form.","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"Note. The @threads call in this analysis takes about 4 hours to complete on a 2018 MacBook Pro.","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"We compare the predictive performance of several tumor growth models on data collected in Laleh et al. (2022) \"Classical mathematical models for prediction of response to chemotherapy and immunotherapy\", PLOS Computational Biology\". In particular, we determine whether differences observed are statistically significant.","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"In addition to classical models, we include a 2D generalization of the General Bertalanffy model, bertalanffy2, and some 1D and 2D neural ODE's. The 2D models still model a single lesion feature, namely it's volume, but add a second latent variable coupled to the volume, effectively making the model second order. For further details, refer to the TumorGrowth.jl package documentation.","category":"page"},{"location":"examples/04_model_battle/notebook/#Conclusions","page":"Model battle","title":"Conclusions","text":"","category":"section"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"We needed to eliminate about 10% of patient records because of failure of the neural network models to converge before parameters went out of bounds. A bootstrap comparison of the differences in mean absolute errors suggest that the General Bertalanffy model performs significantly better than all other models, with of the exception the 1D neural ODE. However, in pair-wise comparisons the neural ODE model was not significantly better than any model. Results are summarised in the table below. Arrows point to bootstrap winners in the top row or first column.","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":" logistic classical_bertalanffy bertalanffy bertalanffy2 1D neural 2D neural\ngompertz draw draw ↑ draw draw ←\nlogistic n/a draw ↑ draw draw ←\nclassical_bertalanffy draw n/a ↑ draw draw ←\nbertalanffy ← ← n/a ← draw ←\nbertalanffy2 draw draw ↑ n/a draw ←\n1D neural draw draw draw draw n/a ←","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"dir = @__DIR__\n\nusing Random\nusing Statistics\nusing TumorGrowth\nusing Lux\nusing Plots\nimport PrettyPrint.pprint\nusing PrettyTables\nusing Bootstrap\nusing Serialization\nusing ProgressMeter\nusing .Threads","category":"page"},{"location":"examples/04_model_battle/notebook/#Data-ingestion","page":"Model battle","title":"Data ingestion","text":"","category":"section"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"Collect together all records with at least 6 measurements, from the data","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"records = filter(patient_data()) do record\n    record.readings >= 6\nend;","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"Here's what a single record looks like:","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"pprint(records[13])","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"@NamedTuple{Pt_hashID::String, Study_Arm::InlineStrings.String15, Study_id::Int64, Arm_id::Int64, T_weeks::Vector{Float64}, T_days::Vector{Int64}, Lesion_diam::Vector{Float64}, Lesion_vol::Vector{Float64}, Lesion_normvol::Vector{Float64}, response::InlineStrings.String7, readings::Int64}(\n  Pt_hashID=\"d9b90f39d6a0b35cbc230adadbd50753-S1\",\n  Study_Arm=InlineStrings.String15(\"Study_1_Arm_1\"),\n  Study_id=1,\n  Arm_id=1,\n  T_weeks=[0.1, 6.0, 12.0, 18.0, \n           24.0, 36.0, 40.0, 42.0, \n           48.0],\n  T_days=[-16, 39, 82, 124, 165, \n          249, 277, 292, 334],\n  Lesion_diam=[17.0, 18.0, 16.0, \n               9.0, 8.0, 9.0, 7.0, \n               7.0, 7.0],\n  Lesion_vol=[2554.76, 3032.64, 2129.92, \n              379.08, 266.24, 379.08, \n              178.36, 178.36, 178.36],\n  Lesion_normvol=[0.000414516882387563, \n                  0.00049205423531127, \n                  0.000345585416295432, \n                  6.15067794139087e-5, \n                  4.3198177036929e-5, \n                  6.15067794139087e-5, \n                  2.89394037571615e-5, \n                  2.89394037571615e-5, \n                  2.89394037571615e-5],\n  response=InlineStrings.String7(\"flux\"),\n  readings=9,\n)","category":"page"},{"location":"examples/04_model_battle/notebook/#Neural-ODEs","page":"Model battle","title":"Neural ODEs","text":"","category":"section"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"We define some one and two-dimensional neural ODE models we want to include in our comparison. The choice of architecture here is somewhat ad hoc and further experimentation might give better results.","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"network = Chain(\n    Dense(1, 3, Lux.tanh, init_weight=Lux.zeros64),\n    Dense(3, 1),\n)\n\nnetwork2 = Chain(\n    Dense(2, 2, Lux.tanh, init_weight=Lux.zeros64),\n    Dense(2, 2),\n)\n\nn1 = neural(Xoshiro(123), network) # `Xoshiro` is a random number generator\nn2 = neural2(Xoshiro(123), network2)","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"Neural2 model, (times, p) -> volumes, where length(p) = 14\n  transform: log","category":"page"},{"location":"examples/04_model_battle/notebook/#Models-to-be-compared","page":"Model battle","title":"Models to be compared","text":"","category":"section"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"model_exs =\n    [:gompertz, :logistic, :classical_bertalanffy, :bertalanffy, :bertalanffy2, :n1, :n2]\nmodels = eval.(model_exs)","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"7-element Vector{Any}:\n gompertz (generic function with 1 method)\n logistic (generic function with 1 method)\n classical_bertalanffy (generic function with 1 method)\n bertalanffy (generic function with 1 method)\n bertalanffy2 (generic function with 1 method)\n neural (12 params)\n neural2 (14 params)","category":"page"},{"location":"examples/04_model_battle/notebook/#Computing-prediction-errors-on-a-holdout-set","page":"Model battle","title":"Computing prediction errors on a holdout set","text":"","category":"section"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"holdouts = 2\nrecs = records;\nerrors = fill(Inf, length(recs), length(models))\n\np = Progress(length(recs))\n\n@threads for i in eachindex(recs)\n    record = records[i]\n    times, volumes = record.T_weeks, record.Lesion_normvol\n    comparison = compare(times, volumes, models; holdouts, flag_out_of_bounds=true)\n    errors[i,:] = TumorGrowth.errors(comparison)\n    next!(p)\nend\nfinish!(p)","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"Progress: 100%|███████████████████████████████████████████████████████████████████████| Time: 3:28:06","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"serialize(joinpath(dir, \"errors.jls\"), errors)","category":"page"},{"location":"examples/04_model_battle/notebook/#Bootstrap-comparisons-(neural-ODE's-excluded)","page":"Model battle","title":"Bootstrap comparisons (neural ODE's excluded)","text":"","category":"section"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"Because the neural ODE errors contain more NaN values (because of parameters going out of bounds), we start with a comparison that excludes them.","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"bad_error_rows = filter(axes(errors, 1)) do i\n    es = errors[i,1:5]\n    any(isnan, es) || any(isinf, es) || max(es...) > 0.1\nend\nproportion_bad = length(bad_error_rows)/size(errors, 1)\n@show proportion_bad","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"0.0171606864274571","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"That's less than 2%. Let's remove them:","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"good_error_rows = setdiff(axes(errors, 1), bad_error_rows);\nerrors = errors[good_error_rows,:];","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"Errors are evidently not normally distributed (and we were not able to transform them to approximately normal):","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"plt = histogram(errors[:, 1], normalize=:pdf, alpha=0.4)\nhistogram!(errors[:, 5], normalize=:pdf, alpha=0.4)\nplt","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"(Image: )","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"savefig(joinpath(dir, \"errors_distribution.png\"))","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"We deem a student t-test inappopriate and instead compute bootstrap confidence intervals for pairwise differences in model errors:","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"confidence_intervals = Array{Any}(undef, length(models) - 2, length(models) - 2)\nfor i in 1:(length(models) - 2)\n    for j in 1:(length(models) - 2)\n        b = bootstrap(\n            mean,\n            errors[:,i] - errors[:,j],\n            BasicSampling(10000),\n        )\n        confidence_intervals[i,j] = only(confint(b, BasicConfInt(0.95)))[2:3]\n    end\nend\nconfidence_intervals","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"5×5 Matrix{Any}:\n (0.0, 0.0)                   (-0.000106171, 4.5993e-5)   (-3.32853e-5, 3.28481e-5)    (3.62648e-5, 0.000197122)  (-0.00101014, -1.12206e-5)\n (-4.66776e-5, 0.000105491)   (0.0, 0.0)                  (-7.49426e-5, 0.000137128)   (1.89733e-5, 0.000258914)  (-0.000996457, 3.65223e-5)\n (-3.1557e-5, 3.36873e-5)     (-0.000135553, 7.8536e-5)   (0.0, 0.0)                   (3.88732e-5, 0.000196808)  (-0.000980241, -2.33476e-5)\n (-0.000196016, -3.84557e-5)  (-0.000258979, -1.9475e-5)  (-0.000197848, -3.71933e-5)  (0.0, 0.0)                 (-0.00109854, -0.000162552)\n (1.096e-5, 0.00101105)       (-4.5169e-5, 0.00100691)    (5.82662e-6, 0.00100553)     (0.000169578, 0.00109791)  (0.0, 0.0)","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"We can interpret the confidence intervals as  follows:","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"if both endpoints -ve, row index wins\nif both endpoints +ve, column index wins\notherwise a draw","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"winner_pointer(ci) = ci == (0, 0) ? \"n/a\" :\n    isnan(first(ci)) && isnan(last(ci)) ? \"inconclusive\" :\n    first(ci) < 0 && last(ci) < 0 ? \"←\" :\n    first(ci) > 0 && last(ci) > 0 ? \"↑\" :\n    \"draw\"\n\ntabular(A, model_exs) = NamedTuple{(:model, model_exs[2:end]...)}((\n    model_exs[1:end-1],\n    (A[1:end-1, j] for j in 2:length(model_exs))...,\n))\n\npretty_table(\n    tabular(winner_pointer.(confidence_intervals), model_exs[1:5]),\n    show_subheader=false,\n)","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"┌───────────────────────┬──────────┬───────────────────────┬─────────────┬──────────────┐\n│                 model │ logistic │ classical_bertalanffy │ bertalanffy │ bertalanffy2 │\n├───────────────────────┼──────────┼───────────────────────┼─────────────┼──────────────┤\n│              gompertz │     draw │                  draw │           ↑ │            ← │\n│              logistic │      n/a │                  draw │           ↑ │         draw │\n│ classical_bertalanffy │     draw │                   n/a │           ↑ │            ← │\n│           bertalanffy │        ← │                     ← │         n/a │            ← │\n└───────────────────────┴──────────┴───────────────────────┴─────────────┴──────────────┘\n","category":"page"},{"location":"examples/04_model_battle/notebook/#Bootstrap-comparison-of-errors-(neural-ODE's-included)","page":"Model battle","title":"Bootstrap comparison of errors (neural ODE's included)","text":"","category":"section"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"bad_error_rows = filter(axes(errors, 1)) do i\n    es = errors[i,:]\n    any(isnan, es) || any(isinf, es) || max(es...) > 0.1\nend\nproportion_bad = length(bad_error_rows)/size(errors, 1)\n@show proportion_bad","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"0.1","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"We remove the additional 10%:","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"good_error_rows = setdiff(axes(errors, 1), bad_error_rows);\nerrors = errors[good_error_rows,:];","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"And proceed as before, but with all columns of errors (all models):","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"confidence_intervals = Array{Any}(undef, length(models), length(models))\nfor i in 1:length(models)\n    for j in 1:length(models)\n        b = bootstrap(\n            mean,\n            errors[:,i] - errors[:,j],\n            BasicSampling(10000),\n        )\n        confidence_intervals[i, j] = only(confint(b, BasicConfInt(0.95)))[2:3]\n    end\nend\npretty_table(\n    tabular(winner_pointer.(confidence_intervals), model_exs),\n    show_subheader=false,\n)","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"┌───────────────────────┬──────────┬───────────────────────┬─────────────┬──────────────┬──────┬────┐\n│                 model │ logistic │ classical_bertalanffy │ bertalanffy │ bertalanffy2 │   n1 │ n2 │\n├───────────────────────┼──────────┼───────────────────────┼─────────────┼──────────────┼──────┼────┤\n│              gompertz │     draw │                  draw │           ↑ │         draw │ draw │  ← │\n│              logistic │      n/a │                  draw │           ↑ │         draw │ draw │  ← │\n│ classical_bertalanffy │     draw │                   n/a │           ↑ │         draw │ draw │  ← │\n│           bertalanffy │        ← │                     ← │         n/a │            ← │ draw │  ← │\n│          bertalanffy2 │     draw │                  draw │           ↑ │          n/a │ draw │  ← │\n│                    n1 │     draw │                  draw │        draw │         draw │  n/a │  ← │\n└───────────────────────┴──────────┴───────────────────────┴─────────────┴──────────────┴──────┴────┘\n","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"","category":"page"},{"location":"examples/04_model_battle/notebook/","page":"Model battle","title":"Model battle","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/02_model_visualization/README/#Contents","page":"Contents","title":"Contents","text":"","category":"section"},{"location":"examples/02_model_visualization/README/","page":"Contents","title":"Contents","text":"file description\nnotebook.ipynb Juptyer notebook (executed)\nnotebook.unexecuted.ipynb Jupyter notebook (unexecuted)\nnotebook.md static markdown (included in MLJFlux.jl docs)\nnotebook.jl executable Julia script annotated with comments\ngenerate.jl maintainers only: execute to generate first 3 from 4th","category":"page"},{"location":"examples/02_model_visualization/README/#Important","page":"Contents","title":"Important","text":"","category":"section"},{"location":"examples/02_model_visualization/README/","page":"Contents","title":"Contents","text":"Scripts or notebooks in this folder cannot be reliably executed without the accompanying Manifest.toml and Project.toml files.","category":"page"},{"location":"quick_start/#Quick-start","page":"Quick start","title":"Quick start","text":"","category":"section"},{"location":"quick_start/","page":"Quick start","title":"Quick start","text":"First, let's grab some real clinical data collected in the study, Laleh et al. (2022) \"Classical mathematical models for prediction of response to chemotherapy and immunotherapy\", PLOS Computational Biology\":","category":"page"},{"location":"quick_start/","page":"Quick start","title":"Quick start","text":"using TumorGrowth\n\nrecords = patient_data();\nrecord = records[16]   # storing all measurements for one lesion\nkeys(record)","category":"page"},{"location":"quick_start/","page":"Quick start","title":"Quick start","text":"Next, we calibrate the General Bertalanffy model using this particular patient record:","category":"page"},{"location":"quick_start/","page":"Quick start","title":"Quick start","text":"times = record.T_weeks\nvolumes = record.Lesion_normvol  # volumes normalized by max over dataset\n\nproblem = CalibrationProblem(times, volumes, bertalanffy)\nsolve!(problem, 2000)  # apply 2000 iterations of the calibration algorithm\np = solution(problem)\npretty(p)","category":"page"},{"location":"quick_start/","page":"Quick start","title":"Quick start","text":"For advanced  options, see CalibrationProblem or the Calibration workflows extended example. ","category":"page"},{"location":"quick_start/","page":"Quick start","title":"Quick start","text":"We can visualize the outcome and make predictions for an extended time period:","category":"page"},{"location":"quick_start/","page":"Quick start","title":"Quick start","text":"using Plots\nplot(problem)","category":"page"},{"location":"quick_start/","page":"Quick start","title":"Quick start","text":"(Image: )","category":"page"},{"location":"quick_start/","page":"Quick start","title":"Quick start","text":"extended_times = vcat(times, [46.0, 53.1])\nbertalanffy(extended_times, p)","category":"page"},{"location":"quick_start/","page":"Quick start","title":"Quick start","text":"And compare several models on a holdout set:","category":"page"},{"location":"quick_start/","page":"Quick start","title":"Quick start","text":"comparison = compare(times, volumes, [bertalanffy, logistic, bertalanffy2], holdouts=2)","category":"page"},{"location":"quick_start/","page":"Quick start","title":"Quick start","text":"plot(comparison)","category":"page"},{"location":"quick_start/","page":"Quick start","title":"Quick start","text":"(Image: )","category":"page"},{"location":"quick_start/","page":"Quick start","title":"Quick start","text":"See compare for more options.","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"EditURL = \"notebook.jl\"","category":"page"},{"location":"examples/03_calibration/notebook/#Calibration-workflows","page":"Calibration workflows","title":"Calibration workflows","text":"","category":"section"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"The code below is also available in notebook form.","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"dir = @__DIR__\n\nusing TumorGrowth\nusing Statistics\nusing Plots\nusing IterationControl\n\nPlots.scalefontsizes() # reset font sizes\nPlots.scalefontsizes(0.85)","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"  Activating project at `~/GoogleDrive/Julia/TumorGrowth/docs/src/examples/03_calibration`\n","category":"page"},{"location":"examples/03_calibration/notebook/#Data-ingestion","page":"Calibration workflows","title":"Data ingestion","text":"","category":"section"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"Get the records which have a least 6 measurements:","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"records = patient_data();\nrecords6 = filter(records) do record\n    record.readings >= 6\nend;","category":"page"},{"location":"examples/03_calibration/notebook/#Helpers","page":"Calibration workflows","title":"Helpers","text":"","category":"section"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"Wrapper to only apply control every 100 steps:","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"sometimes(control) = IterationControl.skip(control, predicate=100)","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"sometimes (generic function with 1 method)","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"Wrapper to only apply control after first 30 steps:","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"warmup(control) = IterationControl.Warmup(control, 30)","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"warmup (generic function with 1 method)","category":"page"},{"location":"examples/03_calibration/notebook/#Patient-A-a-volume-that-is-mostly-decreasiing","page":"Calibration workflows","title":"Patient A - a volume that is mostly decreasiing","text":"","category":"section"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"record = records6[2]","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"(Pt_hashID = \"19ce84cc1b10000b63820280995107c2-S1\", Study_Arm = InlineStrings.String15(\"Study_1_Arm_1\"), Study_id = 1, Arm_id = 1, T_weeks = [0.1, 6.0, 16.0, 24.0, 32.0, 39.0], T_days = [-12, 38, 106, 166, 222, 270], Lesion_diam = [14.0, 10.0, 9.0, 8.0, 8.0, 8.0], Lesion_vol = [1426.88, 520.0, 379.08, 266.24, 266.24, 266.24], Lesion_normvol = [0.000231515230057292, 8.4371439525252e-5, 6.15067794139087e-5, 4.3198177036929e-5, 4.3198177036929e-5, 4.3198177036929e-5], response = InlineStrings.String7(\"down\"), readings = 6)","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"times = record.T_weeks\nvolumes = record.Lesion_normvol;","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"We'll try calibrating the General Bertalanffy model, bertalanffy, with fixed parameter λ=1/5:","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"problem = CalibrationProblem(\n    times,\n    volumes,\n    bertalanffy;\n    frozen=(; λ=1/5),\n    learning_rate=0.001,\n    half_life=21, # place greater weight on recent measurements\n)","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"CalibrationProblem: \n  model: bertalanffy\n  current solution: v0=0.000232  v∞=4.32e-5  ω=0.0432  λ=0.2","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"The controls in the solve! call below have the following interpretations:","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"Step(1): compute 1 iteration at a time\nInvalidValue(): to catch parameters going out of bounds\nNumberLimit(6000): stop after 6000 steps\nGL() |> warmup:  stop using Prechelt's GL criterion after the warm-up period\nNumberSinceBest(10) |> warmup:  stop when it's 10 steps since the best so far\nCallback(prob-> (plot(prob); gui())) |> sometimes: periodically plot the problem","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"Some other possible controls are:","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"TimeLimit(1/60): stop after 1 minute\nWithLossDo(): log to Info the current loss","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"See IterationControl.jl for a complete list.","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"solve!(\n    problem,\n    Step(1),\n    InvalidValue(),\n    NumberLimit(6000),\n    GL() |> warmup,\n    NumberSinceBest(10)  |> warmup,\n    Callback(prob-> (plot(prob); gui())) |> sometimes,\n)\ngui()","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"[ Info: final loss: 5.190581338811233e-10\n[ Info: Stop triggered by NumberLimit(6000) stopping criterion. \n","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"p = solution(problem)\nextended_times = vcat(times, [40.0, 47.0])\nbertalanffy(extended_times, p)","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"8-element Vector{Float64}:\n 0.00022152247503693192\n 0.0001180049861840984\n 5.990483693858979e-5\n 4.433878258915783e-5\n 3.731322416306445e-5\n 3.4181697237280434e-5\n 3.386717191488983e-5\n 3.225813777978713e-5","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"plot(problem, title=\"Patient A, λ=1/5 fixed\", color=:black)\ngui()","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"(Image: )","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"savefig(joinpath(dir, \"patientA.png\"))","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"\"/Users/anthony/GoogleDrive/Julia/TumorGrowth/docs/src/examples/03_calibration/patientA.png\"","category":"page"},{"location":"examples/03_calibration/notebook/#Patient-B-relapse-following-initial-improvement","page":"Calibration workflows","title":"Patient B - relapse following initial improvement","text":"","category":"section"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"record = records6[10]\n\ntimes = record.T_weeks\nvolumes = record.Lesion_normvol;","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"We'll first try the earlier simple model, but we won't freeze λ. Also, we won't specify a half_life, giving all the data equal weight.","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"problem = CalibrationProblem(\n    times,\n    volumes,\n    bertalanffy;\n    learning_rate=0.001,\n)\n\nsolve!(\n    problem,\n    Step(1),\n    InvalidValue(),\n    NumberLimit(6000),\n)\nplot(problem, label=\"bertalanffy\")\ngui()","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"[ Info: final loss: 0.00010518082495583373\n[ Info: Stop triggered by NumberLimit(6000) stopping criterion. \n","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"Let's try the 2D generalization of the General Bertalanffy model:","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"problem = CalibrationProblem(\n    times,\n    volumes,\n    bertalanffy2;\n    learning_rate=0.001,\n)\n\nsolve!(\n    problem,\n    Step(1),\n    InvalidValue(),\n    NumberLimit(6000),\n)\nplot!(problem, label=\"bertalanffy2\")\ngui()","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"(Plot appears below.)","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"[ Info: final loss: 0.0001486220557414735\n[ Info: Stop triggered by NumberLimit(1000) stopping criterion. \n[ Info: final loss: 0.0001486220557414735\n[ Info: Stop triggered by NumberLimit(1000) stopping criterion. \n[ Info: final loss: 1.0981748197354352e-5\n[ Info: Stop triggered by NumberLimit(6000) stopping criterion. \n","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"Here's how we can inspect the final parameters:","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"solution(problem)","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"(v0 = 0.013418165323626751, v∞ = 0.0003759183541925138, ω = 0.12123897563008784, λ = 1.0752692228665386, γ = 0.7251887879657256)","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"Or we can do:","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"solution(problem) |> pretty","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"\"v0=0.0134  v∞=0.000376  ω=0.121  λ=1.08  γ=0.725\"","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"And finally, we'll try a 2D neural ODE model, with fixed volume scale v∞.","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"using Lux, Random","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"Note well the zero-initialization of weights in first layer:","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"network2 = Chain(\n    Dense(2, 2, Lux.tanh, init_weight=Lux.zeros64),\n    Dense(2, 2),\n)","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"Chain(\n    layer_1 = Dense(2 => 2, tanh_fast),  # 6 parameters\n    layer_2 = Dense(2 => 2),            # 6 parameters\n)         # Total: 12 parameters,\n          #        plus 0 states.","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"Notice this network has a total of 12 parameters. To that we'll be adding the initial value u0 of the latent variable. So this is a model with relatively high complexity for this problem.","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"n2 = neural2(Xoshiro(123), network2) # `Xoshiro` is a random number generator","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"Neural2 model, (times, p) -> volumes, where length(p) = 14\n  transform: log","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"Note the reduced learning rate.","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"v∞ = mean(volumes)\n\nproblem = CalibrationProblem(\n    times,\n    volumes,\n    n2;\n    frozen = (; v∞),\n    learning_rate=0.001,\n)\n\nsolve!(\n    problem,\n    Step(1),\n    InvalidValue(),\n    NumberLimit(6000),\n)\nplot!(\n    problem,\n    title = \"Model comparison for Patient B\",\n    label = \"neural2\",\n    legend=:inside,\n)\ngui()","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"[ Info: final loss: 4.9467503956431245e-6\n[ Info: Stop triggered by NumberLimit(6000) stopping criterion. \n","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"(Image: )","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"savefig(joinpath(dir, \"patientB.png\"))","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"\"/Users/anthony/GoogleDrive/Julia/TumorGrowth/docs/src/examples/03_calibration/patientB.png\"","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"For a more principled comparison, we compare the models on a holdout set. We'll additionally throw in 1D neural ODE model.","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"network1 = Chain(\n    Dense(1, 3, Lux.tanh, init_weight=Lux.zeros64),\n    Dense(3, 1),\n)\n\nn1 = neural(Xoshiro(123), network1)\n\nmodels = [bertalanffy, bertalanffy2, n1, n2]\ncalibration_options = [\n    (frozen = (; λ=1/5), learning_rate=0.001, half_life=21), # bertalanffy\n    (frozen = (; λ=1/5), learning_rate=0.001, half_life=21), # bertalanffy2\n    (frozen = (; v∞), learning_rate=0.001, half_life=21), # neural\n    (frozen = (; v∞), learning_rate=0.001, half_life=21), # neural2\n]\nn_iterations = [6000, 6000, 6000, 6000]\ncomparison = compare(times, volumes, models; calibration_options, n_iterations)","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"ModelComparison with 3 holdouts:\n  metric: mae\n  bertalanffy: \t0.004676\n  bertalanffy2: \t0.002197\n  neural (12 params): \t0.004685\n  neural2 (14 params): \t0.004147","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"plot(comparison)\ngui()","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"(Image: )","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"savefig(joinpath(dir, \"patientB_validation.png\"))","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"\"/Users/anthony/GoogleDrive/Julia/TumorGrowth/docs/src/examples/03_calibration/patientB_validation.png\"","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"","category":"page"},{"location":"examples/03_calibration/notebook/","page":"Calibration workflows","title":"Calibration workflows","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/01_data_visualization/notebook/","page":"DATA INGESTION","title":"DATA INGESTION","text":"EditURL = \"notebook.jl\"","category":"page"},{"location":"examples/01_data_visualization/notebook/","page":"DATA INGESTION","title":"DATA INGESTION","text":"using Pkg\n\ndir = @__DIR__\nPkg.activate(dir)\nPkg.instantiate()\n\nusing TumorGrowth\nusing Plots\nlinestyles = [:solid :dash :dot :dashdot :dashdotdot]","category":"page"},{"location":"examples/01_data_visualization/notebook/","page":"DATA INGESTION","title":"DATA INGESTION","text":"1×5 Matrix{Symbol}:\n :solid  :dash  :dot  :dashdot  :dashdotdot","category":"page"},{"location":"examples/01_data_visualization/notebook/#DATA-INGESTION","page":"DATA INGESTION","title":"DATA INGESTION","text":"","category":"section"},{"location":"examples/01_data_visualization/notebook/","page":"DATA INGESTION","title":"DATA INGESTION","text":"Load the Laleh et al (2022) data set as a row table (vector of named tuples):","category":"page"},{"location":"examples/01_data_visualization/notebook/","page":"DATA INGESTION","title":"DATA INGESTION","text":"records = patient_data();","category":"page"},{"location":"examples/01_data_visualization/notebook/","page":"DATA INGESTION","title":"DATA INGESTION","text":"Inspect the field names:","category":"page"},{"location":"examples/01_data_visualization/notebook/","page":"DATA INGESTION","title":"DATA INGESTION","text":"keys(first(records))","category":"page"},{"location":"examples/01_data_visualization/notebook/","page":"DATA INGESTION","title":"DATA INGESTION","text":"(:Pt_hashID, :Study_Arm, :Study_id, :Arm_id, :T_weeks, :T_days, :Lesion_diam, :Lesion_vol, :Lesion_normvol, :response, :readings)","category":"page"},{"location":"examples/01_data_visualization/notebook/","page":"DATA INGESTION","title":"DATA INGESTION","text":"Get the records which have a least 6 measurements and have \"fluctuating\" type:","category":"page"},{"location":"examples/01_data_visualization/notebook/","page":"DATA INGESTION","title":"DATA INGESTION","text":"records6 = filter(records) do record\n    record.readings >= 6 && record.response == \"flux\"\nend;","category":"page"},{"location":"examples/01_data_visualization/notebook/","page":"DATA INGESTION","title":"DATA INGESTION","text":"Plot some of these records:","category":"page"},{"location":"examples/01_data_visualization/notebook/","page":"DATA INGESTION","title":"DATA INGESTION","text":"plt = plot(xlab=\"time\", ylab=\"volume (rescaled by maximum)\")\nfor (i, record) in enumerate(records6[1:5])\n    times = record.T_weeks\n    volumes = record.Lesion_normvol\n    id = string(record.Pt_hashID[1:4], \"…\")\n    max = maximum(volumes)\n    plot!(times, volumes/max, label=id, linestyle=linestyles[i], linecolor=:black)\nend\nplot!(xlab=\"time\", ylab=\"volume\", title = \"Example of fluctuating responses\")\ngui()","category":"page"},{"location":"examples/01_data_visualization/notebook/","page":"DATA INGESTION","title":"DATA INGESTION","text":"(Image: )","category":"page"},{"location":"examples/01_data_visualization/notebook/","page":"DATA INGESTION","title":"DATA INGESTION","text":"savefig(joinpath(dir, \"fluctuating_patient_data.png\"))","category":"page"},{"location":"examples/01_data_visualization/notebook/","page":"DATA INGESTION","title":"DATA INGESTION","text":"\"/Users/anthony/GoogleDrive/Julia/TumorGrowth/docs/src/examples/01_data_visualization/fluctuating_patient_data.png\"","category":"page"},{"location":"examples/01_data_visualization/notebook/","page":"DATA INGESTION","title":"DATA INGESTION","text":"","category":"page"},{"location":"examples/01_data_visualization/notebook/","page":"DATA INGESTION","title":"DATA INGESTION","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/01_data_visualization/README/#Contents","page":"Contents","title":"Contents","text":"","category":"section"},{"location":"examples/01_data_visualization/README/","page":"Contents","title":"Contents","text":"file description\nnotebook.ipynb Juptyer notebook (executed)\nnotebook.unexecuted.ipynb Jupyter notebook (unexecuted)\nnotebook.md static markdown (included in MLJFlux.jl docs)\nnotebook.jl executable Julia script annotated with comments\ngenerate.jl maintainers only: execute to generate first 3 from 4th","category":"page"},{"location":"examples/01_data_visualization/README/#Important","page":"Contents","title":"Important","text":"","category":"section"},{"location":"examples/01_data_visualization/README/","page":"Contents","title":"Contents","text":"Scripts or notebooks in this folder cannot be reliably executed without the accompanying Manifest.toml and Project.toml files.","category":"page"},{"location":"api/#Adding-new-models","page":"Adding new models","title":"Adding new models","text":"","category":"section"},{"location":"api/","page":"Adding new models","title":"Adding new models","text":"Using a custom model is an advanced option outlined below. This section also serves to aid developers who want to permanently add new models to the package.","category":"page"},{"location":"api/","page":"Adding new models","title":"Adding new models","text":"Wherever a model, such as bertalanffy or gompertz, appears in TumorGrowth.jl workflows, any function or other callable mymodel can be used, provided it has the right signature. The required signature is mymodel(times, p; ode_options=...), where times is a vector of times and p parameters of the model in the form of a named tuple, such as p = (; v0=0.03, ω= 1.5); ode_options are options to be passed to the ODE solver (see below). The return value of mymodel will be the corresponding volumes predicted by the model.","category":"page"},{"location":"api/","page":"Adding new models","title":"Adding new models","text":"If the implementation of mymodel requires numerically solving an ordinary differential equation, follow the example given for the bertalanffy2 model, which appears here. (In the TumorGrowth.jl repository, the model ODEs themselves are defined by functions ending in _ode or _ode! in a separate file.)","category":"page"},{"location":"api/","page":"Adding new models","title":"Adding new models","text":"Additionally, one may want to overload some of functions listed below for the new model, especially if convergence during calibration is an issue.  For example, if the new model is a function is called mymodel, and there are two parameters a and b, then you overload guess_parameters like this:","category":"page"},{"location":"api/","page":"Adding new models","title":"Adding new models","text":"function TumorGrowth.guess_parameters(times, volumes, ::typeof(mymodel))\n    < code to guess parameters `a` and `b` >\n\treturn (; a=a, b=b)\nend ","category":"page"},{"location":"api/#Optional-methods-for-new-model-implementations","page":"Adding new models","title":"Optional methods for new model implementations","text":"","category":"section"},{"location":"api/","page":"Adding new models","title":"Adding new models","text":"TumorGrowth.guess_parameters\nTumorGrowth.lower_default\nTumorGrowth.upper_default\nTumorGrowth.frozen_default\nTumorGrowth.optimiser_default\nTumorGrowth.scale_default\nTumorGrowth.penalty_default\nTumorGrowth.radius_default\nTumorGrowth.iterations_default","category":"page"},{"location":"api/#TumorGrowth.guess_parameters","page":"Adding new models","title":"TumorGrowth.guess_parameters","text":"guess_parameters(times, volumes, model)\n\nApply heuristics to guess parameters p for a model.\n\nNew model implementations\n\nFallback returns nothing which will prompt user's to explicitly specify initial parameter values in calibration problems.\n\n\n\n\n\n","category":"function"},{"location":"api/#TumorGrowth.lower_default","page":"Adding new models","title":"TumorGrowth.lower_default","text":"lower_default(model)\n\nReturn a named tuple with the lower bound constraints on parameters for  model.\n\nFor example, a return value of (v0 = 0.1,) indicates that p.v0 > 0.1 is a hard constraint for p, in calls of the form model(times, p), but all other components of p are unconstrained.\n\nNew model implementations\n\nFallback returns NamedTuple().\n\n\n\n\n\n","category":"function"},{"location":"api/#TumorGrowth.upper_default","page":"Adding new models","title":"TumorGrowth.upper_default","text":"upper_default(model)\n\nReturn a named tuple with the upper bound constraints on the parameters for model.\n\nFor example, a return value of (v0 = 1.0,) indicates that p.v0 < 1.0 is a hard constraint for p, in calls of the form model(times, p), but all other components of p are unconstrained.\n\nNew model implementations\n\nFallback returns empty named tuple.\n\n\n\n\n\n","category":"function"},{"location":"api/#TumorGrowth.frozen_default","page":"Adding new models","title":"TumorGrowth.frozen_default","text":"frozen_default(model)\n\nReturn a named tuple indicating parameter values to be frozen by default when calibrating model. A value of nothing for a parameter indicates freezing at initial value.\n\nNew model implementations\n\nFallback returns an empty named tuple.\n\n\n\n\n\n","category":"function"},{"location":"api/#TumorGrowth.optimiser_default","page":"Adding new models","title":"TumorGrowth.optimiser_default","text":"optimiser_default(model)\n\nReturn the default choice of optimiser for model.\n\nNew model implementations\n\nMust return an optimiser from Optimisers.jl, or an optimiser with the same API, or one of the optimisers from LeastSquaresOptim.jl, such as LevenbergMarquardt() or Dogleg().\n\nThe fallback returns Optimisers.Adam(0.0001).\n\n\n\n\n\n","category":"function"},{"location":"api/#TumorGrowth.scale_default","page":"Adding new models","title":"TumorGrowth.scale_default","text":"scale_default(times, volumes, model)\n\nReturn an appropriate default for a function p -> f(p) so that p = f(q) has a value of the same order of magnitude expected for parameters of model, whenever q has the same form as p but with all values equal to one.\n\nIgnored by the optimisers LevenbergMarquardt() and Dogleg().\n\nNew model implementations\n\nFallback returns the identity.\n\n\n\n\n\n","category":"function"},{"location":"api/#TumorGrowth.penalty_default","page":"Adding new models","title":"TumorGrowth.penalty_default","text":"penalty_default(model)\n\nReturn the default loss penalty to be used when calibrating model. The larger the positive value, the more calibration discourages large differences in v0 and v∞ on a log scale. Helps discourage v0 and v∞ drifting out of bounds in models whose ODE have a singularity at the origin.\n\nIgnored by the optimisers LevenbergMarquardt() and Dogleg().\n\nNew model implementations\n\nMust return a value in the range 0 ), and will typically be less than 1.0. Only implement if model has strictly positive parameters named v0 and v∞.\n\nFallback returns 0.\n\n\n\n\n\n","category":"function"},{"location":"api/#TumorGrowth.radius_default","page":"Adding new models","title":"TumorGrowth.radius_default","text":"radius_default(model, optimiser)\n\nReturn the default value of radius when calibrating model using optimiser. This is the initial trust region radius, which is named Δ in LeastSquaresOptim.jl documentation and code.\n\nThis parameter is ignored unless optimiser, as passed to the CalibrationProblem, is LevenbergMarquardt() or Dogleg().\n\nNew model implementations\n\nThe fallback returns:\n\n10.0 if optimiser isaLevenbergMarquardt`\n1.0 if optimiser isaDogleg`\n0 otherwise\n\n\n\n\n\n","category":"function"},{"location":"api/#TumorGrowth.iterations_default","page":"Adding new models","title":"TumorGrowth.iterations_default","text":"iterations_default(model, optimiser)\n\nNumber of iterations, when calibrating model and using optimiser, to be adopted by default in model comparisons. Here optimiser is an optimiser from Optimisers.jl, or implements the same API, or is one of: LevenbergMarquardt(), or Dogleg(). .\n\nNew model implementations\n\nFallback returns 10000, unless optimiser isa Union{LevenbergMarquardt,Dogleg}, in which case 0 is returned (stopping controlled by LeastSquaresOptim.jl).\n\n\n\n\n\n","category":"function"},{"location":"examples/04_model_battle/README/#Contents","page":"Contents","title":"Contents","text":"","category":"section"},{"location":"examples/04_model_battle/README/","page":"Contents","title":"Contents","text":"file description\nnotebook.ipynb Juptyer notebook (executed)\nnotebook.unexecuted.ipynb Jupyter notebook (unexecuted)\nnotebook.md static markdown (included in MLJFlux.jl docs)\nnotebook.jl executable Julia script annotated with comments\ngenerate.jl maintainers only: execute to generate first 3 from 4th","category":"page"},{"location":"examples/04_model_battle/README/#Important","page":"Contents","title":"Important","text":"","category":"section"},{"location":"examples/04_model_battle/README/","page":"Contents","title":"Contents","text":"Scripts or notebooks in this folder cannot be reliably executed without the accompanying Manifest.toml and Project.toml files.","category":"page"},{"location":"examples/02_model_visualization/notebook/","page":"GENERALIZED BERTALANFFY PLOTS: λ = 1/3 (CLASSICAL CASE)","title":"GENERALIZED BERTALANFFY PLOTS: λ = 1/3 (CLASSICAL CASE)","text":"EditURL = \"notebook.jl\"","category":"page"},{"location":"examples/02_model_visualization/notebook/","page":"GENERALIZED BERTALANFFY PLOTS: λ = 1/3 (CLASSICAL CASE)","title":"GENERALIZED BERTALANFFY PLOTS: λ = 1/3 (CLASSICAL CASE)","text":"using Pkg\n\ndir = @__DIR__\nPkg.activate(dir)\nPkg.instantiate()\n\nimport TumorGrowth.bertalanffy\nusing Plots\n\ntimes = range(0, 5, length=200)\none = fill(1, length(times))\nlinestyles = [:solid :dash :dot :dashdot :dashdotdot]","category":"page"},{"location":"examples/02_model_visualization/notebook/","page":"GENERALIZED BERTALANFFY PLOTS: λ = 1/3 (CLASSICAL CASE)","title":"GENERALIZED BERTALANFFY PLOTS: λ = 1/3 (CLASSICAL CASE)","text":"1×5 Matrix{Symbol}:\n :solid  :dash  :dot  :dashdot  :dashdotdot","category":"page"},{"location":"examples/02_model_visualization/notebook/#GENERALIZED-BERTALANFFY-PLOTS:-λ-1/3-(CLASSICAL-CASE)","page":"GENERALIZED BERTALANFFY PLOTS: λ = 1/3 (CLASSICAL CASE)","title":"GENERALIZED BERTALANFFY PLOTS: λ = 1/3 (CLASSICAL CASE)","text":"","category":"section"},{"location":"examples/02_model_visualization/notebook/","page":"GENERALIZED BERTALANFFY PLOTS: λ = 1/3 (CLASSICAL CASE)","title":"GENERALIZED BERTALANFFY PLOTS: λ = 1/3 (CLASSICAL CASE)","text":"p = (v0=1/8, v∞=1.0, ω=1.0, λ=1/3)\nv0s = [1, 1/3, 2/3, 5/4, 3/2]\n\nseries = Array{Float64}(undef, length(times), 5)\nfor (i, v0) in enumerate(v0s)\n    series[:,i] = bertalanffy(times, merge(p, (; v0)))\nend\n\nlabels = map(v0s) do v0\n    r = round(v0, sigdigits=3)\n    \" v0=$r\"\nend |> permutedims\n\nplot(\n    times,\n    series;\n    linecolor=:black,\n    xlab = \"time, t\",\n    ylab = \"volume, v(t)\",\n    labels,\n    linestyles,\n    title=\"Generalized Bertalanffy: v∞=1, ω=1, λ=1/3\"\n)\n\nsavefig(joinpath(dir, \"bertalanffy_varying_v0.png\"))\ngui()","category":"page"},{"location":"examples/02_model_visualization/notebook/","page":"GENERALIZED BERTALANFFY PLOTS: λ = 1/3 (CLASSICAL CASE)","title":"GENERALIZED BERTALANFFY PLOTS: λ = 1/3 (CLASSICAL CASE)","text":"(Image: )","category":"page"},{"location":"examples/02_model_visualization/notebook/#GENERALIZED-BERTALANFFY-PLOTS:-V1/3","page":"GENERALIZED BERTALANFFY PLOTS: λ = 1/3 (CLASSICAL CASE)","title":"GENERALIZED BERTALANFFY PLOTS: V₀=1/3","text":"","category":"section"},{"location":"examples/02_model_visualization/notebook/","page":"GENERALIZED BERTALANFFY PLOTS: λ = 1/3 (CLASSICAL CASE)","title":"GENERALIZED BERTALANFFY PLOTS: λ = 1/3 (CLASSICAL CASE)","text":"p = (v0=1/2, v∞=1.0, ω=1.0, λ=1/3)\nλs = [-1, 0, 1/3, 1]\nseries = Array{Float64}(undef, length(times), 4)\nfor (i, λ) in enumerate(λs)\n    series[:,i] = bertalanffy(times, merge(p, (; λ)))\nend\n\nlabels = map(λs) do λ\n    r = round(λ, sigdigits=3)\n    \"λ=$r\"\nend |> permutedims\n\nplot(\n    times,\n    series;\n    linecolor=:black,\n    xlab = \"time, t\",\n    ylab = \"volume, v(t)\",\n    labels,\n    linestyles,\n    title=\"Generalized Bertalanffy: v0=1/2, v∞=1, ω=1\"\n)\n\nsavefig(joinpath(dir, \"bertalanffy_varying_lambda.png\"))\ngui()","category":"page"},{"location":"examples/02_model_visualization/notebook/","page":"GENERALIZED BERTALANFFY PLOTS: λ = 1/3 (CLASSICAL CASE)","title":"GENERALIZED BERTALANFFY PLOTS: λ = 1/3 (CLASSICAL CASE)","text":"(Image: )","category":"page"},{"location":"examples/02_model_visualization/notebook/","page":"GENERALIZED BERTALANFFY PLOTS: λ = 1/3 (CLASSICAL CASE)","title":"GENERALIZED BERTALANFFY PLOTS: λ = 1/3 (CLASSICAL CASE)","text":"","category":"page"},{"location":"examples/02_model_visualization/notebook/","page":"GENERALIZED BERTALANFFY PLOTS: λ = 1/3 (CLASSICAL CASE)","title":"GENERALIZED BERTALANFFY PLOTS: λ = 1/3 (CLASSICAL CASE)","text":"This page was generated using Literate.jl.","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages   = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [TumorGrowth,]\nPrivate = false\nOrder = [:constant, :type, :function, :macro, :module]","category":"page"},{"location":"reference/#TumorGrowth.CalibrationProblem-Tuple{Any, Any, Any}","page":"Reference","title":"TumorGrowth.CalibrationProblem","text":"    CalibrationProblem(times, volumes, model; learning_rate=0.0001, options...)\n\nSpecify a problem concerned with optimizing the parameters of a tumor growth model, given measured volumes and corresponding times.\n\nSee TumorGrowth for a list of possible models.\n\nDefault optimisation is by Adam gradient descent, using a sum of squares loss. Call solve! on a problem to carry out optimisation, as shown in the example below. See \"Extended Help\" for advanced options, including early stopping.\n\nInitial values of the parameters are inferred by default.\n\nUnless frozen (see \"Extended help\" below), the calibration process learns an initial condition v0 which is generally different from volumes[1].\n\nSimple solve\n\nusing TumorGrowth\n\ntimes = [0.1, 6.0, 16.0, 24.0, 32.0, 39.0]\nvolumes = [0.00023, 8.4e-5, 6.1e-5, 4.3e-5, 4.3e-5, 4.3e-5]\nproblem = CalibrationProblem(times, volumes, gompertz; learning_rate=0.01)\nsolve!(problem, 30)    # apply 30 gradient descent updates\njulia> loss(problem)   # sum of squares loss\n1.7341026729860452e-9\n\np = solution(problem)\njulia> pretty(p)\n\"v0=0.0002261  v∞=2.792e-5  ω=0.05731\"\n\n\nextended_times = vcat(times, [42.0, 46.0])\njulia> gompertz(extended_times, p)[[7, 8]]\n2-element Vector{Float64}:\n 3.374100207406809e-5\n 3.245628908921241e-5\n\nExtended help\n\nSolving with iteration controls\n\nContinuing the example above, we may replace the number of iterations, n, in solve!(problem, n), with any control from IterationControl.jl:\n\nusing IterationControl\nsolve!(\n  problem,\n  Step(1),            # apply controls every 1 iteration...\n  WithLossDo(),       # print loss\n  Callback(problem -> print(pretty(solution(problem)))), # print parameters\n  InvalidValue(),     # stop for ±Inf/NaN loss\n  NumberSinceBest(5), # stop when lowest loss so far was 5 steps prior\n  TimeLimit(1/60),    # stop after one minute\n  NumberLimit(400),   # stop after 400 steps\n)\np = solution(problem)\njulia> loss(problem)\n7.609310030658547e-10\n\nSee IterationControl.jl for all options.\n\nnote: Note\nControlled iteration as above is not recommended if you specify optimiser=LevenbergMarquardt() or optimiser=Dogleg() because the internal state of these optimisers is reset at every Step. Instead, to arrange automatic stopping, use solve!(problem, 0).\n\nVisualizing results\n\nusing Plots\nscatter(times, volumes, xlab=\"time\", ylab=\"volume\", label=\"train\")\nplot!(problem, label=\"prediction\")\n\nKeyword options\n\np0=guess_parameters(times, volumes, model): initial value of the model parameters.\nlower: named tuple indicating lower bounds on components of the model parameter p. For example, if lower=(; v0=0.1), then this introduces the constraint p.v0 < 0.1. The model-specific default value is TumorGrowth.lower_default(model).\nupper: named tuple indicating upper bounds on components of the model parameter p. For example, if upper=(; v0=100), then this introduces the constraint p.v0 < 100. The model-specific default value is TumorGrowth.upper_default(model).\nfrozen: a named tuple, such as (; v0=nothing, λ=1/2); indicating parameters to be frozen at specified values during optimisation; a nothing value means freeze at initial value. The model-specific default value is TumorGrowth.frozen_default(model).\nlearning_rate > 0: learniing rate for Adam gradient descent optimisation. Ignored if optimiser is explicitly specified.\noptimiser: optimisation algorithm, which will be one of two varieties:\nA gradient descent optimiser: This must be from Optimisers.jl or implement the same API.\nA Gauss-Newton optimiser: Either LevenbergMarquardt(), Dogleg(), provided by LeastSquaresOptim.jl (but re-exported by TumorGrowth).\nThe model-specific default value is TumorGrowth.optimiser_default(model), unless learning_rate is specified, in which case it will be Optimisers.Adam(learning_rate).\nscale: a scaling function with the property that p = scale(q) has a value of the same order of magnitude for the model parameters being optimised, whenever q has the same form as a model parameter p but with all values equal to one. Scaling can help components of p converge at a similar rate. Ignored by Gauss-Newton optimisers. Model-specific default is TumorGrowth.scale_default(model).\nradius > 0: initial trust region radius. This is ignored unless optimiser is a Gauss-Newton optimiser. The model-specific default is TumorGrowth.radius_default(model, optmiser), which is typically 10.0 for LevenbergMarquardt() and 1.0 for Dogleg.\nhalf_life=Inf: set to a real positive number to replace the sum of squares loss with a weighted version; weights decay in reverse time with the specified half_life. Ignored by Gauss-Newton optimisers.\npenalty ≥ 0: the larger the positive value, the more a loss penalty discourages large differences in v0 and v∞ on a log scale. Helps discourage v0 and v∞ drifting out of bounds in models whose ODE have a singularity at the origin. Model must include v0 and v∞ as parameters. Ignored by Gauss-Newton optimisers.  The model-specific default value is TumorGrowth.penalty_default(model).\node_options...: optional keyword arguments for the ODE solver, DifferentialEquations.solve, from DifferentialEquations.jl. Not relevant for models using analytic solutions (see the table at TumorGrowth).\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.CalibrationProblem-Tuple{CalibrationProblem}","page":"Reference","title":"TumorGrowth.CalibrationProblem","text":"CalibrationProblem(problem; kwargs...)\n\nConstruct a new calibration problem out an existing problem but supply new keyword arguments, kwargs. Unspecified keyword arguments fall back to defaults, except for p0, which falls back to solution(problem).\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.bertalanffy-Tuple{Any, Any}","page":"Reference","title":"TumorGrowth.bertalanffy","text":"bertalanffy(times, p)\n\nReturn volumes for specified times, based on the analytic solution to the General Bertalanffy model for lesion growth.  Here p will have properties v0, v∞, ω, λ, where v0 is the volume at time times[1]. Other parameters are explained below.\n\nSpecial cases of the model are:\n\nlogistic (λ = -1)\nclassical_bertalanffy  (λ = 1/3)\ngompertz (λ = 0)\n\nUnderlying ODE\n\nIn the General Bertalanffy model, the volume v  0 evolves according to the differential equation\n\ndvdt = ω B_λ(v_v) v\n\nwhere B_λ is the Box-Cox transformation, defined by B_λ(x) = (x^λ - 1)λ, unless λ = 0, in which case, B_λ(x) = log(x). Here:\n\nv_=v∞ is the steady state solution, stable and unique, assuming ω   0; this is sometimes referred to as the carrying capacity\n1ω has the  units of time\nλ is dimensionless\n\nFor a list of all models see TumorGrowth. \n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.bertalanffy2-Tuple{Any, Any}","page":"Reference","title":"TumorGrowth.bertalanffy2","text":"bertalanffy2(times, p; capacity=false, solve_kwargs...)\n\nReturn volumes for specified times, based on numerical solutions to a two-dimensional extension of General Bertalanffy model for lesion growth. Here p will have properties v0, v∞, ω, λ, γ, where v0 is the volume at time times[1].\n\nThe usual General Bertalanffy model is recovered when γ=0. In that case, using bertalanffy, which is based on an analytic solution, may be preferred. Other parameters are explained below.\n\nKeyword options\n\nsolve_kwargs: optional keyword arguments for the ODE solver, DifferentialEquations.solve, from DifferentialEquations.jl.\n\nUnderlying ODE\n\nIn this model the carrying capacity of the bertalanffy model, ordinarily fixed, is introduced as a new latent variable u(t), which is allowed to evolve independently of the volume v(t), at a rate in proportion to its magnitude:\n\ndvdt = ω B_λ(uv) v\n\ndudt = γωu\n\nHere B_λ is the Box-Cox transformation with exponent λ. See bertalanffy. Also:\n\n1ω has units of time\nλ is dimensionless\nγ is dimensionless\n\nSince u is a latent variable, its initial value, v∞ ≡ u(times[1]), is an additional model parameter.\n\nFor a list of all models see TumorGrowth. \n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.bertalanffy_numerical-Tuple{Any, Any}","page":"Reference","title":"TumorGrowth.bertalanffy_numerical","text":"bertalanffy_numerical(times, p; solve_kwargs...)\n\nProvided for testing purposes.\n\nReturn volumes for specified times, based on numerical solutions to the General Bertalanffy model for lesion growth. Here p will have properties v0, v∞, ω, λ, where v0 is the volume at time times[1]; solve_kwargs are optional keyword arguments for the ODE solver, DifferentialEquations.solve, from DifferentialEquations.jl.\n\nSince it is based on analtic solutions, bertalanffy is the preferred alternative to this function.\n\nimportant: Important\nIt is assumed without checking that times is ordered: times == sort(times).\n\nSee also bertalanffy2.\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.classical_bertalanffy-Tuple{Any, Any}","page":"Reference","title":"TumorGrowth.classical_bertalanffy","text":"classical_bertalanffy(times, v0, v∞, ω)\n\nReturn volumes for specified times, based on anaytic solutions to the classical Bertalanffy model for lesion growth. Here p will have properties v0, v∞, ω, where v0 is the volume at time times[1].\n\nThis is the λ=1/3 case of the bertalanffy model.\n\nFor a list of all models see TumorGrowth. \n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.compare-Tuple","page":"Reference","title":"TumorGrowth.compare","text":"compare(times, volumes, models; holdouts=3, metric=mae, advanced_options...)\n\nBy calibrating models using the specified patient times and lesion volumes, compare those models using a hold-out set consisting of the last holdouts data points.\n\ntimes = [0.1, 6.0, 16.0, 24.0, 32.0, 39.0]\nvolumes = [0.00023, 8.4e-5, 6.1e-5, 4.3e-5, 4.3e-5, 4.3e-5]\n\njulia> comparison = compare(times, volumes, [gompertz, logistic])\nModelComparison with 3 holdouts:\n  metric: mae\n  gompertz:     2.198e-6\n  logistic:     6.55e-6\n\njulia> errors(comparison)\n2-element Vector{Float64}:\n 2.197843662660861e-6\n 6.549858321487298e-6\n\njulia> p = parameters(comparison)[1]  # calibrated parameter for `gompertz`\n(v0 = 0.00022643603114569068, v∞ = 3.8453274218216947e-5, ω = 0.11537512108224635)\n\njulia> gompertz(times, p)\n6-element Vector{Float64}:\n 0.00022643603114569068\n 9.435316392754094e-5\n 5.1039159299783234e-5\n 4.303209015899451e-5\n 4.021112910411027e-5\n 3.922743006690166e-5\n\nVisualising comparisons\n\nusing Plots\nplot(comparison, title=\"A comparison of two models\")\n\nKeyword options\n\nholdouts=3: number of time-volume pairs excluded from the end of the calibration data\nmetric=mae: metric applied to holdout set; the reported error on a model predicting volumes v̂ is metric(v̂, v) where v is the last holdouts values of volumes. For example, any regression measure from StatisticalMeasures.jl can be used here. The built-in fallback is mean absolute error.\niterations=TumorGrowth.iterations.(models): a vector of iteration counts for the calibration of models\ncalibration_options: a vector of named tuples providing keyword arguments for the CalibrationProblem for each model. Possible keys are: p0, lower, upper, frozen, learning_rate, optimiser, radius, scale, half_life, penalty, and keys corresponding to any ODE solver options. Keys left unspecified fall back to defaults, as these are described in the CalibrationProblem document string.\n\nSee also errors, parameters.\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.errors-Tuple{ModelComparison}","page":"Reference","title":"TumorGrowth.errors","text":"errors(comparison)\n\nExtract the the vector of errors from a ModelComparison object, as returned by calls to compare.\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.exponential-Tuple{Any, Any}","page":"Reference","title":"TumorGrowth.exponential","text":"exponential(times, p)\n\nReturn volumes for specified times, based on the analytic solution to the exponential model for lesion growth. Here p will have properties v0 and ω, where v0 is the volume at time times[1] and log(2)/ω is the half life. Use negative ω for growth and positive ω for decay.\n\nUnderlying ODE\n\nIn the exponential model, the volume v  0 evolves according to the differential equation\n\ndvdt = -ω v\n\nFor a list of all models see TumorGrowth. \n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.flat_patient_data-Tuple{}","page":"Reference","title":"TumorGrowth.flat_patient_data","text":"flat_patient_data()\n\nReturn, in row table form, the lesion measurement data collected in Laleh et al. (2022) \"Classical mathematical models for prediction of response to chemotherapy and immunotherapy\", PLOS Computational Biology.\n\nEach row represents a single measurement of a single lesion on some day.\n\nSee also patient_data, in which each row represents all measurements of a single lesion.\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.gompertz-Tuple{Any, Any}","page":"Reference","title":"TumorGrowth.gompertz","text":"gompertz(times, p)\n\nReturn volumes for specified times, based on anaytic solutions to the classical Gompertz model for lesion growth. Here p will have properties v0, v∞, ω, where v0 is the volume at time times[1].\n\nThis is the λ=0 case of the bertalanffy model.\n\nFor a list of all models see TumorGrowth. \n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.guess_parameters-Tuple{Any, Any, Any}","page":"Reference","title":"TumorGrowth.guess_parameters","text":"guess_parameters(times, volumes, model)\n\nApply heuristics to guess parameters p for a model.\n\nNew model implementations\n\nFallback returns nothing which will prompt user's to explicitly specify initial parameter values in calibration problems.\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.logistic-Tuple{Any, Any}","page":"Reference","title":"TumorGrowth.logistic","text":"logistic(times, v0, v∞, ω)\n\nReturn volumes for specified times, based on anaytic solutions to the classical logistic (Verhulst) model for lesion growth. Here p will have properties v0, v∞, ω, where v0 is the volume at time times[1].\n\nThis is the λ=-1 case of the bertalanffy model.\n\nFor a list of all models see TumorGrowth. \n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.loss-Tuple","page":"Reference","title":"TumorGrowth.loss","text":"loss(problem)\n\nReturn the sum of squares loss for a calibration problem, as constructed with CalibrationProblem.\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.neural-Tuple","page":"Reference","title":"TumorGrowth.neural","text":"neural([rng,] network; transform=log, inverse=exp)\n\nInitialize the Lux.jl neural network, network, and return a callable object, model, for solving the associated one-dimensional neural ODE for volume growth, as detailed under \"Underlying ODE\" below.\n\nThe returned object, model, is called like this:\n\nvolumes = model(times, p)\n\nwhere p should have properties v0, v∞, θ, where v0 is the initial volume (so that volumes[1] = v0), v∞ is a volume scale parameter, and θ is a network-compatible Lux.jl parameter.\n\nIt seems that calibration works best if v∞ is frozen.\n\nThe form of θ is the same as TumorGrowth.initial_parameters(model), which is also the default initial value used when solving an associated CalibrationProblem.\n\nusing Lux, Random\n\n# define neural network with 1 input and 1 output:\nnetwork = Lux.Chain(Dense(1, 3, Lux.tanh; init_weight=Lux.zeros64), Dense(3, 1))\n\nrng = Xoshiro(123)\nmodel = neural(rng, network)\nθ = TumorGrowth.initial_parameters(model)\ntimes = [0.1, 6.0, 16.0, 24.0, 32.0, 39.0]\nv0, v∞ = 0.00023, 0.00015\np = (; v0, v∞, θ)\n\njulia> volumes = model(times, p) # (constant because of zero-initialization)\n6-element Vector{Float64}:\n 0.00023\n 0.00023\n 0.00023\n 0.00023# # Neural2\n\nUnderlying ODE\n\nView the neural network (with fixed parameter θ) as a mathematical function f and write ϕ for the transform function. Then v(t) = v_ ϕ^-1(y(t)), where y(t) evolves according to\n\ndydt = f(y)\n\nsubject to the initial condition y(t₀) = ϕ(v_0v_), where t₀ is the initial time, times[1]. We are writing v₀=v0 and v_=v∞.\n\nFor a list of all models see TumorGrowth.  See also CalibrationProblem.\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.neural2-Tuple","page":"Reference","title":"TumorGrowth.neural2","text":"neural2([rng,] network; transform=log, inverse=exp)\n\nInitialize the Lux.jl neural network, network, and return a callable object, model, for solving the associated two-dimensional neural ODE for volume growth, as detailed under \"Underlying ODE\" below.\n\nThe returned object model is called like this:\n\nvolumes = model(times, p)\n\nwhere p should have properties v0, v∞, θ, where v0 is the initial volume (so that volumes[1] = v0), v∞ is a volume scale parameter, and θ is a network-compatible Lux.jl parameter.\n\nIt seems that calibration works best if v∞ is frozen.\n\nThe form of θ is the same as TumorGrowth.initial_parameters(model), which is also the default initial value used when solving an associated CalibrationProblem.\n\nusing Lux, Random\n\n# define neural network with 2 inputs and 2 outputs:\nnetwork = Lux.Chain(Dense(2, 3, Lux.tanh; init_weight=Lux.zeros64), Dense(3, 2))\n\nrng = Xoshiro(123)\nmodel = neural2(rng, network)\nθ = TumorGrowth.initial_parameters(model)\ntimes = [0.1, 6.0, 16.0, 24.0, 32.0, 39.0]\nv0, v∞ = 0.00023, 0.00015\np = (; v0, v∞, θ)\n\njulia> volumes = model(times, p) # (constant because of zero-initialization)\n6-element Vector{Float64}:\n 0.00023\n 0.00023\n 0.00023\n 0.00023\n 0.00023\n 0.00023\n\nUnderlying ODE\n\nView the neural network (with fixed parameter θ) as a mathematical function f, with components f₁ and f₂, and write ϕ for the transform function. Then v(t) = v_ ϕ^-1(y(t)), where y(t), and a latent variable u(t), evolve according to\n\ndydt = f₁(y u)\n\ndudt = f₂(y u)\n\nsubject to the initial conditions y(t₀) = ϕ(v₀v_), u(t₀) = 1, where t₀ is the initial time, times[1]. We are writing v₀=v0 and v_=v∞.\n\nFor a list of all models see TumorGrowth.  See also CalibrationProblem.\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.parameters-Tuple{ModelComparison}","page":"Reference","title":"TumorGrowth.parameters","text":"parameters(comparison)\n\nExtract the the vector of parameters from a ModelComparison object, as returned by calls to compare.\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.patient_data-Tuple{}","page":"Reference","title":"TumorGrowth.patient_data","text":"patient_data()\n\nReturn, in row table form, the lesion measurement data collected in Laleh et al. (2022) \"Classical mathematical models for prediction of response to chemotherapy and immunotherapy\", PLOS Computational Biology.\n\nEach row represents all measurements for a single lesion for a unique patient.\n\nrecord = first(patient_data())\n\njulia> record.Pt_hashID # patient identifier\n\"0218075314855e6ceacca856fcd4c737-S1\"\n\njulia> record.T_weeks # measure times, in weeks\n7-element Vector{Float64}:\n  0.1\n  6.0\n 12.0\n 17.0\n 23.0\n 29.0\n 35.0\n\njulia> record.Lesion_normvol # all volumes measured, normalised by dataset max\n7-element Vector{Float64}:\n 0.000185364052636979\n 0.00011229838600811\n 8.4371439525252e-5\n 8.4371439525252e-5\n 1.05464299406565e-5\n 2.89394037571615e-5\n 8.4371439525252e-5\n\nSee also flat_patient_data.\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.solution-Tuple{CalibrationProblem}","page":"Reference","title":"TumorGrowth.solution","text":"solution(problem)\n\nReturn to the solution to a CalibrationProblem. Normally applied after calling solve!(problem).\n\nAlso returns the solution to internally defined problems, as constructed with TumorGrowth.OptimisationProblem, TumorGrowth.CurveOptimisationProblem.\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.solve!-Tuple","page":"Reference","title":"TumorGrowth.solve!","text":"solve!(problem, n)\n\nSolve a calibration problem, as constructed with CalibrationProblem. The calibrated parameters are then returned by solution(problem).\n\nIf using a Gauss-Newton optimiser (LevenbergMarquardt or Dogleg) specify n=0 to choose n automatically.\n\n\n\nsolve!(problem, controls...)\n\nSolve a calibration problem using one or more iteration controls, from the package IterationControls.jl. See the \"Extended help\" section of CalibrationProblem for examples.\n\nNot recommended for Gauss-Newton optimisers (LevenbergMarquardt or Dogleg).\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.TumorGrowth","page":"Reference","title":"TumorGrowth.TumorGrowth","text":"TumorGrowth.jl provides the following models for tumor growth:\n\nmodel description parameters, p analytic?\nbertalanffy General Bertalanffy (GB) (; v0, v∞, ω, λ) yes\nbertalanffy_numerical General Bertalanffy (testing only) (; v0, v∞, ω, λ) no\nbertalanffy2 2D extension of General Bertalanffy (; v0, v∞, ω, λ, γ) no\ngompertz classical Gompertz (GB, λ=0) (; v0, v∞, ω) yes\nlogistic classical Logistic/Verhulst (GB, λ=-1) (; v0, v∞, ω) yes\nclassical_bertalanffy classical Bertalanffy (GB, λ=1/3) (; v0, v∞, ω) yes\nexponential exponential decay or growth (; v0, ω) yes\nneural(rng, network) 1D neural ODE with Lux.jl network (; v0, v∞, θ) no\nneural2(rng, network) 2D neural ODE with Lux.jl network (; v0, v∞, θ) no\n\nHere a model is a callable object, that outputs a sequence of lesion volumes, given times, by solving a related ordinary differential equation with parameters (p below):\n\nusing TumorGrowth\n\ntimes = times = [0.1, 6.0, 16.0, 24.0, 32.0, 39.0]\np = (v0=0.0002261, v∞=2.792e-5,  ω=0.05731) # `v0` is the initial volume\nvolumes = gompertz(times, p)\n6-element Vector{Float64}:\n 0.0002261\n 0.0001240760197801191\n 6.473115210101774e-5\n 4.751268597529182e-5\n 3.9074807723757934e-5\n 3.496675045077041e-5\n\nIn every model, v0 is the initial volume, so that volumes[1] == v0.\n\nIn the case analytic solutions to the underlying ODEs are not known, optional keyword arguments for the DifferentialEquations.jl solver can be passed to the model call.\n\nTumorGrowth.jl also provides a CalibrationProblem tool to calibrate model parameters, and a compare tool to compare models on a holdout set.\n\n\n\n\n\n","category":"module"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [TumorGrowth,]\nPublic = false\nOrder = [:constant, :type, :function, :macro, :module]","category":"page"},{"location":"reference/#TumorGrowth.WeightedL2Loss","page":"Reference","title":"TumorGrowth.WeightedL2Loss","text":"WeightedL2Loss(times, h=Inf)\n\nPrivate method.\n\nReturn a weighted sum of squares loss function (ŷ, y) -> loss, where the weights decay in reverse time with a half life h.\n\n\n\n\n\n","category":"type"},{"location":"reference/#TumorGrowth.bertalanffy2_ode!-NTuple{4, Any}","page":"Reference","title":"TumorGrowth.bertalanffy2_ode!","text":"bertalanffy2_ode!(dX, X, p, t)\n\nA two-dimensional extension of the ODE describing the General Bertalanffy model for lesion growth.  Here X = [v, u], where v is volume at time t and u is the \"carrying capacity\" at time t, a latent variable. The time derivatives are written to dX. For the specific form of the ODE, see bertalanffy2.\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.bertalanffy_ode-Tuple{Any, Any, Any}","page":"Reference","title":"TumorGrowth.bertalanffy_ode","text":"bertalanffy_ode(v, p, t)\n\nBased on the General Bertalanffy model, return the rate in change in volume at time t, for a current volume of v. For details, see bertalanffy.\n\nNote here that v, and the return value, are vectors with a single element, rather than scalars.\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.curvature-Union{Tuple{T}, Tuple{AbstractArray{T}, Any}} where T","page":"Reference","title":"TumorGrowth.curvature","text":"curvature(xs, ys)\n\nReturn the coefficient a for the parabola x -> a*x^2 + b*x + c of best fit, for ordinates xs and coordinates ys.\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.delete-Tuple{NamedTuple, Any}","page":"Reference","title":"TumorGrowth.delete","text":"delete(x, kys)\n\nPrivate method.\n\nAssuming x is a named tuple, return a copy of x with any key in kys removed. Otherwise, assuming x is a structured object (such as a ComponentArray) first convert to a named tuple and then delete the specified keys.\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.fill_gaps-Tuple{Any, Any, Any}","page":"Reference","title":"TumorGrowth.fill_gaps","text":"fill_gaps(short, long, filler)\n\nPrivate method.\n\nHere long is a ComponentArray and short a named tuple with some of the keys from long. The method returns a ComponentArray with the same structure as long but with the values of short merged into long, with all other (possibly nested) values replaced with filler for numerical values or arrays of Inf in the case of array values.\n\n``julia long = (a=1, b=rand(1,2), c=(d=4, e=rand(2))) |> ComponentArray short = (; a=10) # could alternatively be aComponentArray`\n\njulia> TumorGrowth.fill_gaps(short, long, Inf) filled = ComponentVector{Float64}(a = 10.0, b = [Inf Inf], c = (d = Inf, e = [Inf, Inf]))\n\njulia> all(filled .> long) true\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.force_constraints!-NTuple{4, Any}","page":"Reference","title":"TumorGrowth.force_constraints!","text":"force_constraints!(x_candidate, x, lower, upper)\n\nPrivate method.\n\nAssumes x is a ComponentArray for which TumorGrowth.satisfies_constraints(x, lower, upper) is true. The method mutates those components of the x_candidate which do not satisfy the constraints by moving from x towards the boundary half the distance to the boundary, along the failed component.\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.frozen_default-Tuple{Any}","page":"Reference","title":"TumorGrowth.frozen_default","text":"frozen_default(model)\n\nReturn a named tuple indicating parameter values to be frozen by default when calibrating model. A value of nothing for a parameter indicates freezing at initial value.\n\nNew model implementations\n\nFallback returns an empty named tuple.\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.functor-Tuple{Any, Any}","page":"Reference","title":"TumorGrowth.functor","text":"function TumorGrowth.functor(x, frozen)\n\nPrivate method.\n\nFor a ComponentArray, x, return a tuple (xfree, reconstructor), where:\n\nxfree is a deconstructed version of x with entries corresponding to keys in the ordinary named tuple frozen deleted.\nreconstruct is a method to reconstruct a ComponentArray from something similar to xfree, ensuring the missing keys get values from the named tuple frozen, as demonstrated in the example below. You can also apply reconstruct to things like xfree wrapped as ComponentArrays.\n\n\nc = (x =1, y=2, z=3) |> ComponentArray\nfree, reconstruct = TumorGrowth.functor(c, (; y=20))\njulia> free\n(x = 1, z = 3)\n\njulia> reconstruct((x=100, z=300))\nComponentVector{Int64}(x = 100, y = 20, z = 300)\n\njulia> reconstruct(ComponentArray(x=100, z=300)))\nComponentVector{Int64}(x = 100, y = 20, z = 300)\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.functor-Tuple{Any}","page":"Reference","title":"TumorGrowth.functor","text":"TumorGrowth.functor(x) -> destructured_x, recover\n\nPrivate method.\n\nAn extension of Functors.functor from the package Functors.jl, with an overloading for ComponentArrays.\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.iterations_default-Tuple{Any, Any}","page":"Reference","title":"TumorGrowth.iterations_default","text":"iterations_default(model, optimiser)\n\nNumber of iterations, when calibrating model and using optimiser, to be adopted by default in model comparisons. Here optimiser is an optimiser from Optimisers.jl, or implements the same API, or is one of: LevenbergMarquardt(), or Dogleg(). .\n\nNew model implementations\n\nFallback returns 10000, unless optimiser isa Union{LevenbergMarquardt,Dogleg}, in which case 0 is returned (stopping controlled by LeastSquaresOptim.jl).\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.lower_default-Tuple{Any}","page":"Reference","title":"TumorGrowth.lower_default","text":"lower_default(model)\n\nReturn a named tuple with the lower bound constraints on parameters for  model.\n\nFor example, a return value of (v0 = 0.1,) indicates that p.v0 > 0.1 is a hard constraint for p, in calls of the form model(times, p), but all other components of p are unconstrained.\n\nNew model implementations\n\nFallback returns NamedTuple().\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.merge-Tuple{Any, Any}","page":"Reference","title":"TumorGrowth.merge","text":"TumorGrowth.merge(x, y::NamedTuple)\n\nPrivate method.\n\nOrdinary merge if x is also a named tuple. More generally, first deconstruct x using TumorGrowth.functor, merge as usual, and reconstruct.\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.neural_ode-Tuple{Any, Any}","page":"Reference","title":"TumorGrowth.neural_ode","text":"neural_ode([rng,] network)\n\nInitialize the Lux.jl neural2 network, network, and return an associated ODE, ode, with calling syntax dX_dt = ode(X, p, t), where p is a network-compatible parameter.\n\nThe initialized parameter value can be recovered with TumorGrowth.initial_parameters(ode). Get the network state with TumorGrowth.state(ode).\n\nusing Lux\nusing Random\n\nrng = Xoshiro(123)\nnetwork = network = Lux.Chain(Lux.Dense(2, 3, Lux.tanh), Lux.Dense(3, 2))\node = neural_ode(rng, network)\nθ = TumorGrowth.initial_parameters(ode)\node(rand(2), θ, 42.9) # last argument irrelevant as `ode` is autonomous\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.optimiser_default-Tuple{Any}","page":"Reference","title":"TumorGrowth.optimiser_default","text":"optimiser_default(model)\n\nReturn the default choice of optimiser for model.\n\nNew model implementations\n\nMust return an optimiser from Optimisers.jl, or an optimiser with the same API, or one of the optimisers from LeastSquaresOptim.jl, such as LevenbergMarquardt() or Dogleg().\n\nThe fallback returns Optimisers.Adam(0.0001).\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.penalty_default-Tuple{Any}","page":"Reference","title":"TumorGrowth.penalty_default","text":"penalty_default(model)\n\nReturn the default loss penalty to be used when calibrating model. The larger the positive value, the more calibration discourages large differences in v0 and v∞ on a log scale. Helps discourage v0 and v∞ drifting out of bounds in models whose ODE have a singularity at the origin.\n\nIgnored by the optimisers LevenbergMarquardt() and Dogleg().\n\nNew model implementations\n\nMust return a value in the range 0 ), and will typically be less than 1.0. Only implement if model has strictly positive parameters named v0 and v∞.\n\nFallback returns 0.\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.radius_default-Tuple{Any, Any}","page":"Reference","title":"TumorGrowth.radius_default","text":"radius_default(model, optimiser)\n\nReturn the default value of radius when calibrating model using optimiser. This is the initial trust region radius, which is named Δ in LeastSquaresOptim.jl documentation and code.\n\nThis parameter is ignored unless optimiser, as passed to the CalibrationProblem, is LevenbergMarquardt() or Dogleg().\n\nNew model implementations\n\nThe fallback returns:\n\n10.0 if optimiser isaLevenbergMarquardt`\n1.0 if optimiser isaDogleg`\n0 otherwise\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.recover-Tuple{Any, Any}","page":"Reference","title":"TumorGrowth.recover","text":"recover(tuple, from)\n\nPrivate method.\n\nReturn a new named tuple by replacing any nothing values with the corresponding value in the from named tuple, whenever a corresponding key exists, and otherwise ignore.\n\njulia> recover((x=1, y=nothing, z=3, w=nothing), (x=10, y=2, k=7))\n(x = 1, y = 2, z = 3, w = nothing)\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.satisfies_constraints-Tuple{Any, Any, Any}","page":"Reference","title":"TumorGrowth.satisfies_constraints","text":"satisfies_constraints(x, lower, upper)\n\nPrivate method.\n\nReturns true if both of the following are true:\n\nupper.k < x.k for each k appearing as a key of upper\nx.k < lower.k for each k appearing as a key of lower\n\nOtherwise, returns false.\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.scale_default-Tuple{Any, Any, Any}","page":"Reference","title":"TumorGrowth.scale_default","text":"scale_default(times, volumes, model)\n\nReturn an appropriate default for a function p -> f(p) so that p = f(q) has a value of the same order of magnitude expected for parameters of model, whenever q has the same form as p but with all values equal to one.\n\nIgnored by the optimisers LevenbergMarquardt() and Dogleg().\n\nNew model implementations\n\nFallback returns the identity.\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.slope-Union{Tuple{T}, Tuple{AbstractArray{T}, Any}} where T","page":"Reference","title":"TumorGrowth.slope","text":"slope(xs, ys)\n\nReturn the slope of the line of least-squares best fit for ordinates xs and coordinates ys.\n\n\n\n\n\n","category":"method"},{"location":"reference/#TumorGrowth.upper_default-Tuple{Any}","page":"Reference","title":"TumorGrowth.upper_default","text":"upper_default(model)\n\nReturn a named tuple with the upper bound constraints on the parameters for model.\n\nFor example, a return value of (v0 = 1.0,) indicates that p.v0 < 1.0 is a hard constraint for p, in calls of the form model(times, p), but all other components of p are unconstrained.\n\nNew model implementations\n\nFallback returns empty named tuple.\n\n\n\n\n\n","category":"method"},{"location":"calibration/#Calibration","page":"Calibration","title":"Calibration","text":"","category":"section"},{"location":"calibration/","page":"Calibration","title":"Calibration","text":"For a basic example, see the CalibrationProblem document string below. For an extended workflow, see Calibration workflows.","category":"page"},{"location":"calibration/#Overview","page":"Calibration","title":"Overview","text":"","category":"section"},{"location":"calibration/","page":"Calibration","title":"Calibration","text":"By default, calibration is performed using a gradient descent optimiser to minimise a least-squares error on provided clinical measurements, and uses the adjoint method to auto-differentiate solutions to the underlying ODE's, with respect to the ODE parameters, and initial conditions to be optimised. Alternatively, Levengerg-Marquardt or Powell's dog leg optimisation may be employed. This can be faster for the smaller models, but enjoys fewer features.","category":"page"},{"location":"calibration/#Gradient-descent-optimisation","page":"Calibration","title":"Gradient descent optimisation","text":"","category":"section"},{"location":"calibration/","page":"Calibration","title":"Calibration","text":"Calibration using a gradient descent optimiser has these advantages:","category":"page"},{"location":"calibration/","page":"Calibration","title":"Calibration","text":"Any updater (optimiser) from Optimisers.jl can be used.\nFine-grained control of iteration, including live plots, is possible using IterationControl.jl.\nStability issues for models with a singularity at zero volume can be mitigated by specifying a loss penalty.\nInstead of minimizing a least-squares loss, one can give more weight to recent observations by specifying an appropriate half_life.\nConvergence may be faster for models with a large number of parameters (e.g., larger neural ODE models)\nModels can be calibrated even if the number of observations is less than the number of model parameters.","category":"page"},{"location":"calibration/","page":"Calibration","title":"Calibration","text":"By default, optimisation is by gradient descent using Adaptive Moment Estimation and a user-specifiable learning_rate.","category":"page"},{"location":"calibration/#Levengerg-Marquardt-/-dog-leg-optimisation","page":"Calibration","title":"Levengerg-Marquardt / dog leg optimisation","text":"","category":"section"},{"location":"calibration/","page":"Calibration","title":"Calibration","text":"The main advantage of these methods is that they are faster for all the models currently provided, with the exception of large neural ODE models. Users can specify an initial trust_region_radius to mitigate instability.","category":"page"},{"location":"calibration/#Usage","page":"Calibration","title":"Usage","text":"","category":"section"},{"location":"calibration/","page":"Calibration","title":"Calibration","text":"CalibrationProblem","category":"page"},{"location":"calibration/#TumorGrowth.CalibrationProblem","page":"Calibration","title":"TumorGrowth.CalibrationProblem","text":"    CalibrationProblem(times, volumes, model; learning_rate=0.0001, options...)\n\nSpecify a problem concerned with optimizing the parameters of a tumor growth model, given measured volumes and corresponding times.\n\nSee TumorGrowth for a list of possible models.\n\nDefault optimisation is by Adam gradient descent, using a sum of squares loss. Call solve! on a problem to carry out optimisation, as shown in the example below. See \"Extended Help\" for advanced options, including early stopping.\n\nInitial values of the parameters are inferred by default.\n\nUnless frozen (see \"Extended help\" below), the calibration process learns an initial condition v0 which is generally different from volumes[1].\n\nSimple solve\n\nusing TumorGrowth\n\ntimes = [0.1, 6.0, 16.0, 24.0, 32.0, 39.0]\nvolumes = [0.00023, 8.4e-5, 6.1e-5, 4.3e-5, 4.3e-5, 4.3e-5]\nproblem = CalibrationProblem(times, volumes, gompertz; learning_rate=0.01)\nsolve!(problem, 30)    # apply 30 gradient descent updates\njulia> loss(problem)   # sum of squares loss\n1.7341026729860452e-9\n\np = solution(problem)\njulia> pretty(p)\n\"v0=0.0002261  v∞=2.792e-5  ω=0.05731\"\n\n\nextended_times = vcat(times, [42.0, 46.0])\njulia> gompertz(extended_times, p)[[7, 8]]\n2-element Vector{Float64}:\n 3.374100207406809e-5\n 3.245628908921241e-5\n\nExtended help\n\nSolving with iteration controls\n\nContinuing the example above, we may replace the number of iterations, n, in solve!(problem, n), with any control from IterationControl.jl:\n\nusing IterationControl\nsolve!(\n  problem,\n  Step(1),            # apply controls every 1 iteration...\n  WithLossDo(),       # print loss\n  Callback(problem -> print(pretty(solution(problem)))), # print parameters\n  InvalidValue(),     # stop for ±Inf/NaN loss\n  NumberSinceBest(5), # stop when lowest loss so far was 5 steps prior\n  TimeLimit(1/60),    # stop after one minute\n  NumberLimit(400),   # stop after 400 steps\n)\np = solution(problem)\njulia> loss(problem)\n7.609310030658547e-10\n\nSee IterationControl.jl for all options.\n\nnote: Note\nControlled iteration as above is not recommended if you specify optimiser=LevenbergMarquardt() or optimiser=Dogleg() because the internal state of these optimisers is reset at every Step. Instead, to arrange automatic stopping, use solve!(problem, 0).\n\nVisualizing results\n\nusing Plots\nscatter(times, volumes, xlab=\"time\", ylab=\"volume\", label=\"train\")\nplot!(problem, label=\"prediction\")\n\nKeyword options\n\np0=guess_parameters(times, volumes, model): initial value of the model parameters.\nlower: named tuple indicating lower bounds on components of the model parameter p. For example, if lower=(; v0=0.1), then this introduces the constraint p.v0 < 0.1. The model-specific default value is TumorGrowth.lower_default(model).\nupper: named tuple indicating upper bounds on components of the model parameter p. For example, if upper=(; v0=100), then this introduces the constraint p.v0 < 100. The model-specific default value is TumorGrowth.upper_default(model).\nfrozen: a named tuple, such as (; v0=nothing, λ=1/2); indicating parameters to be frozen at specified values during optimisation; a nothing value means freeze at initial value. The model-specific default value is TumorGrowth.frozen_default(model).\nlearning_rate > 0: learniing rate for Adam gradient descent optimisation. Ignored if optimiser is explicitly specified.\noptimiser: optimisation algorithm, which will be one of two varieties:\nA gradient descent optimiser: This must be from Optimisers.jl or implement the same API.\nA Gauss-Newton optimiser: Either LevenbergMarquardt(), Dogleg(), provided by LeastSquaresOptim.jl (but re-exported by TumorGrowth).\nThe model-specific default value is TumorGrowth.optimiser_default(model), unless learning_rate is specified, in which case it will be Optimisers.Adam(learning_rate).\nscale: a scaling function with the property that p = scale(q) has a value of the same order of magnitude for the model parameters being optimised, whenever q has the same form as a model parameter p but with all values equal to one. Scaling can help components of p converge at a similar rate. Ignored by Gauss-Newton optimisers. Model-specific default is TumorGrowth.scale_default(model).\nradius > 0: initial trust region radius. This is ignored unless optimiser is a Gauss-Newton optimiser. The model-specific default is TumorGrowth.radius_default(model, optmiser), which is typically 10.0 for LevenbergMarquardt() and 1.0 for Dogleg.\nhalf_life=Inf: set to a real positive number to replace the sum of squares loss with a weighted version; weights decay in reverse time with the specified half_life. Ignored by Gauss-Newton optimisers.\npenalty ≥ 0: the larger the positive value, the more a loss penalty discourages large differences in v0 and v∞ on a log scale. Helps discourage v0 and v∞ drifting out of bounds in models whose ODE have a singularity at the origin. Model must include v0 and v∞ as parameters. Ignored by Gauss-Newton optimisers.  The model-specific default value is TumorGrowth.penalty_default(model).\node_options...: optional keyword arguments for the ODE solver, DifferentialEquations.solve, from DifferentialEquations.jl. Not relevant for models using analytic solutions (see the table at TumorGrowth).\n\n\n\n\n\nCalibrationProblem(problem; kwargs...)\n\nConstruct a new calibration problem out an existing problem but supply new keyword arguments, kwargs. Unspecified keyword arguments fall back to defaults, except for p0, which falls back to solution(problem).\n\n\n\n\n\n","category":"type"},{"location":"examples/03_calibration/README/#Contents","page":"Contents","title":"Contents","text":"","category":"section"},{"location":"examples/03_calibration/README/","page":"Contents","title":"Contents","text":"file description\nnotebook.ipynb Juptyer notebook (executed)\nnotebook.unexecuted.ipynb Jupyter notebook (unexecuted)\nnotebook.md static markdown (included in MLJFlux.jl docs)\nnotebook.jl executable Julia script annotated with comments\ngenerate.jl maintainers only: execute to generate first 3 from 4th","category":"page"},{"location":"examples/03_calibration/README/#Important","page":"Contents","title":"Important","text":"","category":"section"},{"location":"examples/03_calibration/README/","page":"Contents","title":"Contents","text":"Scripts or notebooks in this folder cannot be reliably executed without the accompanying Manifest.toml and Project.toml files.","category":"page"},{"location":"#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"TumorGrowth","category":"page"},{"location":"#TumorGrowth","page":"Overview","title":"TumorGrowth","text":"TumorGrowth.jl provides the following models for tumor growth:\n\nmodel description parameters, p analytic?\nbertalanffy General Bertalanffy (GB) (; v0, v∞, ω, λ) yes\nbertalanffy_numerical General Bertalanffy (testing only) (; v0, v∞, ω, λ) no\nbertalanffy2 2D extension of General Bertalanffy (; v0, v∞, ω, λ, γ) no\ngompertz classical Gompertz (GB, λ=0) (; v0, v∞, ω) yes\nlogistic classical Logistic/Verhulst (GB, λ=-1) (; v0, v∞, ω) yes\nclassical_bertalanffy classical Bertalanffy (GB, λ=1/3) (; v0, v∞, ω) yes\nexponential exponential decay or growth (; v0, ω) yes\nneural(rng, network) 1D neural ODE with Lux.jl network (; v0, v∞, θ) no\nneural2(rng, network) 2D neural ODE with Lux.jl network (; v0, v∞, θ) no\n\nHere a model is a callable object, that outputs a sequence of lesion volumes, given times, by solving a related ordinary differential equation with parameters (p below):\n\nusing TumorGrowth\n\ntimes = times = [0.1, 6.0, 16.0, 24.0, 32.0, 39.0]\np = (v0=0.0002261, v∞=2.792e-5,  ω=0.05731) # `v0` is the initial volume\nvolumes = gompertz(times, p)\n6-element Vector{Float64}:\n 0.0002261\n 0.0001240760197801191\n 6.473115210101774e-5\n 4.751268597529182e-5\n 3.9074807723757934e-5\n 3.496675045077041e-5\n\nIn every model, v0 is the initial volume, so that volumes[1] == v0.\n\nIn the case analytic solutions to the underlying ODEs are not known, optional keyword arguments for the DifferentialEquations.jl solver can be passed to the model call.\n\nTumorGrowth.jl also provides a CalibrationProblem tool to calibrate model parameters, and a compare tool to compare models on a holdout set.\n\n\n\n\n\n","category":"module"},{"location":"","page":"Overview","title":"Overview","text":"See Quick start for further details.","category":"page"}]
}
